{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 分布的形式化\n",
    "\n",
    "## 物理意义\n",
    "\n",
    "[Gumbel](https://en.wikipedia.org/wiki/Gumbel_distribution) 分布是一种极值型分布。举例而言，假设每次测量心率值为一个随机变量（服从某种[指数族分布](https://en.wikipedia.org/wiki/Exponential_family)，如正态分布），每天测量10次心率并取最大的一个心率值作为当天的心率测量值。显然，每天纪录的心率值也是一个随机变量，并且它的概率分布即为 Gumbel 分布。\n",
    "\n",
    "\n",
    "## 概率密度函数（PDF）\n",
    "\n",
    "Gumbel 分布的 PDF 如下：\n",
    "\n",
    "$$f(x;\\mu,\\beta) = e^{-z-e^{-z}},\\ z= \\frac{x - \\mu}{\\beta}$$\n",
    "\n",
    "公式中，$\\mu$ 是位置系数（Gumbel 分布的众数是 $\\mu$），$\\beta$ 是尺度系数（Gumbel 分布的方差是 $\\frac{\\pi^2}{6}\\beta^2$）。\n",
    "\n",
    "![PDF](https://upload.wikimedia.org/wikipedia/commons/thumb/3/32/Gumbel-Density.svg/488px-Gumbel-Density.svg.png)\n",
    "**Gumble PDF 示例图【[src](https://en.wikipedia.org/wiki/Gumbel_distribution)】**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.183939720586\n"
     ]
    }
   ],
   "source": [
    "def gumbel_pdf(x, mu=0, beta=1):\n",
    "    z = (x - mu) / beta\n",
    "    return np.exp(-z - np.exp(-z)) / beta\n",
    "\n",
    "print(gumbel_pdf(0.5, 0.5, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 累计密度函数（CDF）\n",
    "相应的，gumbel 分布的 CDF 的公式如下：\n",
    "\n",
    "$$F(z;\\mu,\\beta) = e^{-e^{-(x-\\mu)/\\beta}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.899965162661\n"
     ]
    }
   ],
   "source": [
    "def gumbel_cdf(x, mu=0, beta=1):\n",
    "    z = (x - mu) / beta\n",
    "    return np.exp(-np.exp(-z))\n",
    "    \n",
    "print(gumbel_cdf(5, 0.5, 2))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CDF 的反函数\n",
    "根据 CDF 容易得到其反函数：\n",
    "\n",
    "$$F^{-1}(y;\\mu,\\beta) = \\mu - \\beta \\ln(-\\ln(y))$$\n",
    "\n",
    "我们可以利用反函数法和生成服从 Gumbel 分布的随机数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "[[ 2.26084029  1.69630763 -0.71364075]\n",
      " [ 1.1703834   1.65582117  2.7283681 ]]\n"
     ]
    }
   ],
   "source": [
    "def inv_gumbel_cdf(y, mu=0, beta=1, eps=1e-20):\n",
    "    return mu - beta * np.log(-np.log(y + eps))\n",
    "\n",
    "print(inv_gumbel_cdf(gumbel_cdf(5, 0.5, 2), 0.5, 2))\n",
    "\n",
    "def sample_gumbel(shape):\n",
    "    p = np.random.random(shape)\n",
    "    return inv_gumbel_cdf(p)\n",
    "\n",
    "print(sample_gumbel([2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Gumbel-Max\n",
    "\n",
    "Gumbel 随机数可以用来对多项分布进行采样。\n",
    "\n",
    "## 2.1 基于 softmax 的采样\n",
    "\n",
    "首先来看常规的采样方法。\n",
    "\n",
    "对于 $logits = (x_1, \\dots, x_K)$，首先利用 softmax 运算得到规一化的概率分布（多项分布）。\n",
    "\n",
    "$$\\pi_k = \\frac{exp\\{x_k\\}}{\\sum_{k\\prime=1}^{K} exp\\{x_{k\\prime}\\}}$$\n",
    "\n",
    "然后，利用轮盘赌的方式采样。下面的代码，直接使用 numpy 的 [choice](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html) 函数实现。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    max_value = np.max(logits)\n",
    "    exp = np.exp(logits - max_value)\n",
    "    exp_sum = np.sum(exp)\n",
    "    dist = exp / exp_sum\n",
    "    return dist\n",
    "\n",
    "def roulette(p):\n",
    "    p = np.asarray(p)\n",
    "    cdf = p.cumsum()\n",
    "    r = np.random.random()\n",
    "    for i in range(len(cdf)):\n",
    "        if r <= cdf[i]: break\n",
    "    return i\n",
    "\n",
    "def sample_with_softmax(logits, size):\n",
    "    '''\n",
    "    pros = softmax(logits)\n",
    "\n",
    "    ret = np.empty(np.product(size)).astype('int')\n",
    "    for i in range(len(ret)):\n",
    "        ret[i] = roulette(pros)\n",
    "        \n",
    "    return ret.reshape(size)\n",
    "    '''\n",
    "    \n",
    "    pros = softmax(logits)\n",
    "    return np.random.choice(len(logits), size, p=pros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 基于 gumbel 的采样（gumbel-max）\n",
    "对于某组 logits，生成相同数量的 gumbel 随机数，并加到 logits 上。 然后选择数值最大的元素的编号作为采样值。\n",
    "示例代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_with_gumbel_noise(logits, size):\n",
    "    noise = sample_gumbel((size, len(logits)))\n",
    "    return np.argmax(logits + noise, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以[证明](https://hips.seas.harvard.edu/blog/2013/04/06/the-gumbel-max-trick-for-discrete-distributions/)，gumbel-max 方法的采样效果等效于基于 softmax 的方式。下面的实验直观地展示两种方法的采样效果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  4358.,  22962.,   7143.,   6761.,   3638.,   5848.,   5946.,\n",
       "         15969.,   9951.,  17424.]),\n",
       " array([ 0. ,  0.9,  1.8,  2.7,  3.6,  4.5,  5.4,  6.3,  7.2,  8.1,  9. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0efc35e1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1111)\n",
    "logits = (np.random.random(10) - 0.5) * 2  # (-1, 1)\n",
    "    \n",
    "pop = 100000\n",
    "softmax_samples = sample_with_softmax(logits, pop)\n",
    "gumbel_samples = sample_with_gumbel_noise(logits, pop)\n",
    "        \n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(softmax_samples)\n",
    "    \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(gumbel_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于 Gumbel 随机数可以预先计算好，采样过程也不需要计算 softmax，因此，某些情况下，gumbel-max 方法相比于 softmax，在采样速度上会有优势。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gumbel-Softmax\n",
    "\n",
    "如果仅仅是提供一种常规 softmax 采样的替代方案， gumbel 分布似乎应用价值并不大。幸运的是，我们可以利用 gumbel 实现多项分布采样的 reparameterization（再参数化）。\n",
    "\n",
    "在介绍 [VAE](http://blog.csdn.net/jackytintin/article/details/53641885) 的时候讨论过，为了实现端到端的训练，VAE 采用的一个再参数化的技巧对高斯分布进行采样:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def guassian_sample(shape):\n",
    "    epsilon = K.random_normal(shape, mean=0.,std=1) # 标准高斯分布\n",
    "    z = z_mean + exp(z_log_var / 2) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在介绍的 VAE 里，假设隐变量（latent variables）服从标准正态分布。下面将会看到，利用 gumbel-softmax 技巧，我们可以将隐变量建模为服从离散的多项分布。\n",
    "\n",
    "在上面的示例中，sample_with_softmax 使用了 choise 操作，而这个操作是不可导的。同样，观察 sample_with_gumbel_noise, armmax 操作同样不可导。然而，argmax 有一个 soft 版本，即 **softmax**。\n",
    "\n",
    "我们首先扩展上面定义的 softmax 函数，添加一个 temperature 参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generalized_softmax(logits, temperature=1):\n",
    "    logits /= temperature\n",
    "    return softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temperature 是在大于零的参数，它控制着 softmax 的 soft 程度。温度越高，生成的分布越平滑；温度越低，生成的分布越接近离散的 one-hot 分布。下面示例对比了不同温度下，softmax 的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 10 artists>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3cwsg5zczcwy6P8DQVHMs3jM6NAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0ecb2b2b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1111)\n",
    "n = 10\n",
    "logits = (np.random.random(n) - 0.5) * 2  # (-1, 1)\n",
    "x = range(n)\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "t = .1\n",
    "plt.bar(x, generalized_softmax(logits, t))\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "t = 5\n",
    "plt.bar(x, generalized_softmax(logits, t))\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "t = 50\n",
    "plt.bar(x, generalized_softmax(logits, t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 gumbel-max 中的 argmax 操作，替换为 softmax，便实现了对于原来不可导的采样操作的软化版的近似。训练中，可以通过逐渐降低温度，以逐步逼近真实的离散分布。利用 gumbel-softmax，VAE 的隐变量可以用多项分布进行建模，一个示例见 [repo](https://github.com/DingKe/ml-tutorial/tree/master/gumbel)。这里，仅展示一个 toy 示例（代码[来自](http://amid.fish/humble-gumbel)）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: \n",
      "0.02 0.37 1.00 0.37 0.02\n",
      "Distribution mean: 2.00\n",
      "Distribution mean: 2.13\n",
      "Distribution mean: 2.23\n",
      "Distribution mean: 2.60\n",
      "Distribution mean: 2.75\n",
      "Distribution mean: 3.23\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "def differentiable_sample(logits, temperature=1):\n",
    "    noise = tf.random_uniform(tf.shape(logits), seed=11)\n",
    "    logits_with_noise = logits - tf.log(-tf.log(noise))\n",
    "    return tf.nn.softmax(logits_with_noise / temperature)\n",
    "\n",
    "mean = tf.Variable(2.)\n",
    "idxs = tf.Variable([0., 1., 2., 3., 4.])\n",
    "# An unnormalised approximately-normal distribution\n",
    "logits = tf.exp(-(idxs - mean) ** 2)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def print_logit_vals():\n",
    "    logit_vals = sess.run(logits)\n",
    "    print(\" \".join([\"{:.2f}\"] * len(logit_vals)).format(*logit_vals))\n",
    "    \n",
    "print(\"Logits: \")\n",
    "print_logit_vals()\n",
    "\n",
    "sample = differentiable_sample(logits)\n",
    "sample_weights = tf.Variable([1., 2., 3., 4., 5.], trainable=False)\n",
    "result = tf.reduce_sum(sample * sample_weights)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=1).minimize(-result)\n",
    "\n",
    "print(\"Distribution mean: {:.2f}\".format(sess.run(mean)))\n",
    "for i in range(5):\n",
    "    sess.run(train_op)\n",
    "    print(\"Distribution mean: {:.2f}\".format(sess.run(mean)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，利用 gumbel-softmax 训练参数向着预期的方向改变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: \n",
      "0.02 0.37 1.00 0.37 0.02\n",
      "Distribution mean: 2.00\n",
      "Distribution mean: 2.32\n",
      "Distribution mean: 2.65\n",
      "Distribution mean: 2.87\n",
      "Distribution mean: 3.10\n",
      "Distribution mean: 3.36\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "mean = tf.Variable(2.)\n",
    "idxs = tf.Variable([0., 1., 2., 3., 4.])\n",
    "# An unnormalised approximately-normal distribution\n",
    "logits = tf.exp(-(idxs - mean) ** 2)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def print_logit_vals():\n",
    "    logit_vals = sess.run(logits)\n",
    "    print(\" \".join([\"{:.2f}\"] * len(logit_vals)).format(*logit_vals))\n",
    "    \n",
    "print(\"Logits: \")\n",
    "print_logit_vals()\n",
    "\n",
    "sample = tf.nn.softmax(logits)\n",
    "sample_weights = tf.Variable([1., 2., 3., 4., 5.], trainable=False)\n",
    "result = tf.reduce_sum(sample * sample_weights)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=1).minimize(-result)\n",
    "\n",
    "print(\"Distribution mean: {:.2f}\".format(sess.run(mean)))\n",
    "for i in range(5):\n",
    "    sess.run(train_op)\n",
    "    print(\"Distribution mean: {:.2f}\".format(sess.run(mean)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讨论\n",
    "乍看起来，gumbel-softmax 的用处令人费解。比如上面的代码示例，直接使用 softmax，也可以达到类似的参数训练效果。但两者有着根本的区别。\n",
    "原理上，常规的 softmax 直接建模了一个概率分布（多项分布），基于交叉熵的训练准则使分布尽可能靠近目标分布；而 gumbel-softmax 则是对多项分布采样的一个近似。使用上，常规的有监督学习任务（分类器训练）中，直接学习输出的概率分布是自然的选择；而对于涉及采样的学习任务（VAE 隐变量采样、强化学习中对actions 集合进行采样以确定下一步的操作），gumbel-softmax 提供了一种再参数化的方法，使得模型可以以端到端的方式进行训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. http://amid.fish/humble-gumbel\n",
    "2. [proof of Gumbel based sampling](https://hips.seas.harvard.edu/blog/2013/04/06/the-gumbel-max-trick-for-discrete-distributions/)\n",
    "3. https://blog.evjang.com/2016/11/tutorial-categorical-variational.html\n",
    "4. Jang et al. [CATEGORICAL REPARAMETERIZATION WITH GUMBEL-SOFTMAX](https://arxiv.org/abs/1611.01144).\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
