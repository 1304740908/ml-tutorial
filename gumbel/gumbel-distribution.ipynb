{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 分布的形式化\n",
    "\n",
    "## 物理意义\n",
    "\n",
    "[Gumbel](https://en.wikipedia.org/wiki/Gumbel_distribution) 分布是一种极值型分布。举例而言，假设每次测量心率值为一个随机变量（服从某种[指数族分布](https://en.wikipedia.org/wiki/Exponential_family)，如正态分布），每天测量10次心率并取最大的一个心率值作为当天的心率测量值。显然，每天纪录的心率值也是一个随机变量，并且它的概率分布即为 Gumbel 分布。\n",
    "\n",
    "\n",
    "## 概率密度函数（PDF）\n",
    "\n",
    "Gumbel 分布的 PDF 如下：\n",
    "\n",
    "$$f(x;\\mu,\\beta) = e^{-z-e^{-z}},\\ z= \\frac{x - \\mu}{\\beta}$$\n",
    "\n",
    "公式中，$\\mu$ 是位置系数（Gumbel 分布的众数是 $\\mu$），$\\beta$ 是尺度系数（Gumbel 分布的方差是 $\\frac{\\pi^2}{6}\\beta^2$）。\n",
    "\n",
    "![PDF](https://upload.wikimedia.org/wikipedia/commons/thumb/3/32/Gumbel-Density.svg/488px-Gumbel-Density.svg.png)\n",
    "**Gumble PDF 示例图【[src](https://en.wikipedia.org/wiki/Gumbel_distribution)】**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.183939720586\n"
     ]
    }
   ],
   "source": [
    "def gumbel_pdf(x, mu=0, beta=1):\n",
    "    z = (x - mu) / beta\n",
    "    return np.exp(-z - np.exp(-z)) / beta\n",
    "\n",
    "print(gumbel_pdf(0.5, 0.5, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 累计密度函数（CDF）\n",
    "相应的，gumbel 分布的 CDF 的公式如下：\n",
    "\n",
    "$$F(z;\\mu,\\beta) = e^{-e^{-(x-\\mu)/\\beta}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.899965162661\n"
     ]
    }
   ],
   "source": [
    "def gumbel_cdf(x, mu=0, beta=1):\n",
    "    z = (x - mu) / beta\n",
    "    return np.exp(-np.exp(-z))\n",
    "    \n",
    "print(gumbel_cdf(5, 0.5, 2))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CDF 的反函数\n",
    "根据 CDF 容易得到其反函数：\n",
    "\n",
    "$$F^{-1}(y;\\mu,\\beta) = \\mu - \\beta \\ln(-\\ln(y))$$\n",
    "\n",
    "我们可以利用反函数法和生成服从 Gumbel 分布的随机数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "[[ 0.29569995  0.02471482 -1.7583011 ]\n",
      " [-0.25833806  0.10539504  2.66052767]]\n"
     ]
    }
   ],
   "source": [
    "def inv_gumbel_cdf(y, mu=0, beta=1, eps=1e-20):\n",
    "    return mu - beta * np.log(-np.log(y + eps))\n",
    "\n",
    "print(inv_gumbel_cdf(gumbel_cdf(5, 0.5, 2), 0.5, 2))\n",
    "\n",
    "def sample_gumbel(shape):\n",
    "    p = np.random.random(shape)\n",
    "    return inv_gumbel_cdf(p)\n",
    "\n",
    "print(sample_gumbel([2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Gumbel-Max\n",
    "\n",
    "Gumbel 随机数可以用来对多项分布进行采样。\n",
    "\n",
    "## 2.1 基于 softmax 的采样\n",
    "\n",
    "首先来看常规的采样方法。\n",
    "\n",
    "对于 $logits = (x_1, \\dots, x_K)$，首先利用 softmax 运算得到规一化的概率分布（多项分布）。\n",
    "\n",
    "$$\\pi_k = \\frac{e^{x_k}}{\\sum_{k^\\prime=1}^{K} e^{x_{k^\\prime}}}$$\n",
    "\n",
    "然后，利用轮盘赌的方式采样。下面的代码，直接使用 numpy 的 [choice](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html) 函数实现。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    max_value = np.max(logits)\n",
    "    exp = np.exp(logits - max_value)\n",
    "    exp_sum = np.sum(exp)\n",
    "    dist = exp / exp_sum\n",
    "    return dist\n",
    "\n",
    "def roulette(p):\n",
    "    p = np.asarray(p)\n",
    "    cdf = p.cumsum()\n",
    "    r = np.random.random()\n",
    "    for i in range(len(cdf)):\n",
    "        if r <= cdf[i]: break\n",
    "    return i\n",
    "\n",
    "def sample_with_softmax(logits, size):\n",
    "    '''\n",
    "    pros = softmax(logits)\n",
    "\n",
    "    ret = np.empty(np.product(size)).astype('int')\n",
    "    for i in range(len(ret)):\n",
    "        ret[i] = roulette(pros)\n",
    "        \n",
    "    return ret.reshape(size)\n",
    "    '''\n",
    "    \n",
    "    pros = softmax(logits)\n",
    "    return np.random.choice(len(logits), size, p=pros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 基于 gumbel 的采样（gumbel-max）\n",
    "对于某组 logits，生成相同数量的 gumbel 随机数，并加到 logits 上。 然后选择数值最大的元素的编号作为采样值。\n",
    "示例代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_with_gumbel_noise(logits, size):\n",
    "    noise = sample_gumbel((size, len(logits)))\n",
    "    return np.argmax(logits + noise, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以[证明](https://hips.seas.harvard.edu/blog/2013/04/06/the-gumbel-max-trick-for-discrete-distributions/)，gumbel-max 方法的采样效果等效于基于 softmax 的方式。下面的实验直观地展示两种方法的采样效果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  4358.,  22962.,   7143.,   6761.,   3638.,   5848.,   5946.,\n",
       "         15969.,   9951.,  17424.]),\n",
       " array([ 0. ,  0.9,  1.8,  2.7,  3.6,  4.5,  5.4,  6.3,  7.2,  8.1,  9. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa3785ebe50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1111)\n",
    "logits = (np.random.random(10) - 0.5) * 2  # (-1, 1)\n",
    "    \n",
    "pop = 100000\n",
    "softmax_samples = sample_with_softmax(logits, pop)\n",
    "gumbel_samples = sample_with_gumbel_noise(logits, pop)\n",
    "        \n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(softmax_samples)\n",
    "    \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(gumbel_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于 Gumbel 随机数可以预先计算好，采样过程也不需要计算 softmax，因此，某些情况下，gumbel-max 方法相比于 softmax，在采样速度上会有优势。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gumbel-Softmax\n",
    "\n",
    "如果仅仅是提供一种常规 softmax 采样的替代方案， gumbel 分布似乎应用价值并不大。幸运的是，我们可以利用 gumbel 实现多项分布采样的 reparameterization（再参数化）。\n",
    "\n",
    "在介绍 [VAE](http://blog.csdn.net/jackytintin/article/details/53641885) 的时候讨论过，为了实现端到端的训练，VAE 采用的一个再参数化的技巧对高斯分布进行采样:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def guassian_sample(shape):\n",
    "    epsilon = K.random_normal(shape, mean=0.,std=1) # 标准高斯分布\n",
    "    z = z_mean + exp(z_log_var / 2) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在介绍的 VAE 里，假设隐变量（latent variables）服从标准正态分布。下面将会看到，利用 gumbel-softmax 技巧，我们可以将隐变量建模为服从离散的多项分布。\n",
    "\n",
    "在上面的示例中，sample_with_softmax 使用了 choise 操作，而这个操作是不可导的。同样，观察 sample_with_gumbel_noise, armmax 操作同样不可导。然而，argmax 有一个 soft 版本，即 **softmax**。\n",
    "\n",
    "我们首先扩展上面定义的 softmax 函数，添加一个 temperature 参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generalized_softmax(logits, temperature=1):\n",
    "    logits = logits / temperature\n",
    "    return softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temperature 是在大于零的参数，它控制着 softmax 的 soft 程度。温度越高，生成的分布越平滑；温度越低，生成的分布越接近离散的 one-hot 分布。下面示例对比了不同温度下，softmax 的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 10 artists>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM0uQw93MLEH/Hy9HeZRuzKXVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa347c99690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1111)\n",
    "n = 10\n",
    "logits = (np.random.random(n) - 0.5) * 2  # (-1, 1)\n",
    "x = range(n)\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "t = .1\n",
    "plt.bar(x, generalized_softmax(logits, t))\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "t = 1\n",
    "plt.bar(x, generalized_softmax(logits, t))\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "t = 50\n",
    "plt.bar(x, generalized_softmax(logits, t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 gumbel-max 中的 argmax 操作，替换为 softmax，便实现了对于原来不可导的采样操作的软化版的近似。训练中，可以通过逐渐降低温度，以逐步逼近真实的离散分布。利用 gumbel-softmax，VAE 的隐变量可以用多项分布进行建模，一个示例见 [repo](https://github.com/DingKe/ml-tutorial/tree/master/gumbel)。这里，仅展示一个 toy 示例（代码[来自](http://amid.fish/humble-gumbel)）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: \n",
      "0.02 0.37 1.00 0.37 0.02\n",
      "Distribution mean: 2.00\n",
      "Distribution mean: 2.13\n",
      "Distribution mean: 2.23\n",
      "Distribution mean: 2.60\n",
      "Distribution mean: 2.75\n",
      "Distribution mean: 3.23\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "def differentiable_sample(logits, temperature=1):\n",
    "    noise = tf.random_uniform(tf.shape(logits), seed=11)\n",
    "    logits_with_noise = logits - tf.log(-tf.log(noise))\n",
    "    return tf.nn.softmax(logits_with_noise / temperature)\n",
    "\n",
    "mean = tf.Variable(2.)\n",
    "idxs = tf.Variable([0., 1., 2., 3., 4.])\n",
    "# An unnormalised approximately-normal distribution\n",
    "logits = tf.exp(-(idxs - mean) ** 2)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def print_logit_vals():\n",
    "    logit_vals = sess.run(logits)\n",
    "    print(\" \".join([\"{:.2f}\"] * len(logit_vals)).format(*logit_vals))\n",
    "    \n",
    "print(\"Logits: \")\n",
    "print_logit_vals()\n",
    "\n",
    "sample = differentiable_sample(logits)\n",
    "sample_weights = tf.Variable([1., 2., 3., 4., 5.], trainable=False)\n",
    "result = tf.reduce_sum(sample * sample_weights)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=1).minimize(-result)\n",
    "\n",
    "print(\"Distribution mean: {:.2f}\".format(sess.run(mean)))\n",
    "for i in range(5):\n",
    "    sess.run(train_op)\n",
    "    print(\"Distribution mean: {:.2f}\".format(sess.run(mean)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，利用 gumbel-softmax 训练参数向着预期的方向改变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: \n",
      "0.02 0.37 1.00 0.37 0.02\n",
      "Distribution mean: 2.00\n",
      "Distribution mean: 2.32\n",
      "Distribution mean: 2.65\n",
      "Distribution mean: 2.87\n",
      "Distribution mean: 3.10\n",
      "Distribution mean: 3.36\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "mean = tf.Variable(2.)\n",
    "idxs = tf.Variable([0., 1., 2., 3., 4.])\n",
    "# An unnormalised approximately-normal distribution\n",
    "logits = tf.exp(-(idxs - mean) ** 2)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def print_logit_vals():\n",
    "    logit_vals = sess.run(logits)\n",
    "    print(\" \".join([\"{:.2f}\"] * len(logit_vals)).format(*logit_vals))\n",
    "    \n",
    "print(\"Logits: \")\n",
    "print_logit_vals()\n",
    "\n",
    "sample = tf.nn.softmax(logits)\n",
    "sample_weights = tf.Variable([1., 2., 3., 4., 5.], trainable=False)\n",
    "result = tf.reduce_sum(sample * sample_weights)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=1).minimize(-result)\n",
    "\n",
    "print(\"Distribution mean: {:.2f}\".format(sess.run(mean)))\n",
    "for i in range(5):\n",
    "    sess.run(train_op)\n",
    "    print(\"Distribution mean: {:.2f}\".format(sess.run(mean)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讨论\n",
    "乍看起来，gumbel-softmax 的用处令人费解。比如上面的代码示例，直接使用 softmax，也可以达到类似的参数训练效果。但两者有着根本的区别。\n",
    "原理上，常规的 softmax 直接建模了一个概率分布（多项分布），基于交叉熵的训练准则使分布尽可能靠近目标分布；而 gumbel-softmax 则是对多项分布采样的一个近似。使用上，常规的有监督学习任务（分类器训练）中，直接学习输出的概率分布是自然的选择；而对于涉及采样的学习任务（VAE 隐变量采样、强化学习中对actions 集合进行采样以确定下一步的操作），gumbel-softmax 提供了一种再参数化的方法，使得模型可以以端到端的方式进行训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. http://amid.fish/humble-gumbel\n",
    "2. [proof of Gumbel based sampling](https://hips.seas.harvard.edu/blog/2013/04/06/the-gumbel-max-trick-for-discrete-distributions/)\n",
    "3. https://blog.evjang.com/2016/11/tutorial-categorical-variational.html\n",
    "4. Jang et al. [CATEGORICAL REPARAMETERIZATION WITH GUMBEL-SOFTMAX](https://arxiv.org/abs/1611.01144).\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
